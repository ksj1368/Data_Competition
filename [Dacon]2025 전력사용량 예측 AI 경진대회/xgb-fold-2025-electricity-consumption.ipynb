{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256547233,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport xgboost\nfrom xgboost import XGBRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\nimport optuna\nimport gc\nfrom datetime import datetime\nfrom pathlib import Path\nprint(\"pandas version:\", pd.__version__)  # 2.2.3\nprint(\"numpy version:\", np.__version__)   # 1.26.4\nprint(\"optuna version:\", optuna.__version__)   # 1.26.4\nprint(\"xgboost version:\", xgboost.__version__)   # 2.0.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 평가산신, scaling 함수 \ndef SMAPE(true, pred):\n    return np.mean((np.abs(true - pred)) / (np.abs(true) + np.abs(pred))) * 200\n\ndef weighted_mse(alpha=3):\n    def weighted_mse_fixed(label, pred):\n        residual = (label - pred).astype(\"float\")\n        grad = np.where(residual > 0, -2 * alpha * residual, -2 * residual)\n        hess = np.where(residual > 0, 2 * alpha, 2.0)\n        return grad, hess\n    return weighted_mse_fixed\n\ndef inv_boxcox(y, lam, boxcox_offset):\n    if lam == 0:\n        return np.exp(y) - boxcox_offset\n    else:\n        return np.power(y * lam + 1, 1 / lam) - boxcox_offset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 데이터 로드\ntrain_df = pd.read_csv('/kaggle/input/preprocess-2025-electricity-consumption/prep_train.csv')\ntest_df = pd.read_csv('/kaggle/input/preprocess-2025-electricity-consumption/prep_test.csv')\nsub_df = pd.read_csv('/kaggle/input/preprocess-2025-electricity-consumption/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 이상치 제거\nout_date_dict = {\n    5: ['5_20240804 07', '5_20240804 08'],\n    8: ['8_20240721 08'],\n    12: ['12_20240721 08', '12_20240721 09', '12_20240721 10', '12_20240721 11'],\n    30: ['30_20240713 20', '30_20240725 00'],\n    40: ['40_20240714 00'],\n    41: ['41_20240622 01', '41_20240622 04', '41_20240717 14', '41_20240717 15'],\n    42: ['42_20240717 14'],\n    43: ['43_20240610 17', '43_20240610 18', '43_20240812 16', '43_20240812 17'],\n    44: ['44_20240630 00', '44_20240630 02', '44_20240606 13', '44_20240606 14'],\n    52: ['52_20240810 00', '52_20240810 02'],\n    53: ['53_20240615 08', '53_20240615 11'],\n    67: ['67_20240610 17', '67_20240610 18', '67_20240812 16', '67_20240812 17'],\n    68: ['68_20240628 23', '68_20240629 01'],\n    70: ['70_20240605 09', '70_20240603 11', '70_20240603 12'],\n    72: ['72_20240721 11'],\n    76: ['76_20240603 13', '76_20240620 12', '76_20240620 16'],\n    79: ['79_20240819 04', '79_20240819 03', '79_20240819 05'],\n    80: ['80_20240720 10', '80_20240720 11', '80_20240720 12', '80_20240706 10', '80_20240706 13', '80_20240706 14'],\n    81: ['81_20240717 14'],\n    90: ['90_20240605 18'],\n    92: ['92_20240717 18', '92_20240717 19', '92_20240717 21'],\n    94: ['94_20240727 09', '94_20240727 12'],\n    97: ['97_20240605 05'],\n}\n\nall_remove_values = []\nfor date_times in out_date_dict.values():\n    all_remove_values.extend(date_times)\n\ntrain_df = train_df[~train_df['num_date_time'].isin(all_remove_values)]\nprint(f\"Train shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")\nprint(f\"Submission shape: {sub_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# target 스케일링(Box-Cox)\ntarget_col = '전력소비량(kWh)'\nboxcox_offset = 1\ntrain_df[target_col], lam = stats.boxcox(train_df[target_col] + boxcox_offset)\n\n\n# 범주형 변환\ncategorical_cols = [\n    '건물번호','건물유형', 'is_holiday', 'is_building_holiday','peak_flag','rain_flag',\n    'discomfort_category','heat_wave_flag','tropical_night_flag','cluster_id', 'is_weekday', 'business_hours'\n]\n\nfor col in categorical_cols:\n    if col in train_df.columns:\n        train_df[col] = train_df[col].astype('category')\n    if col in test_df.columns:\n        test_df[col] = test_df[col].astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Select\nexclude_cols = ['일시', 'num_date_time', 'is_weekday', 'time_period', 'business_hours', 'rain_flag', 'max_days',\n                'high_temp_flag', 'wind_chill_effect', 'temp_change_hour', 'temp_change_day', '기온_C_diff1', \n                '기온_C_diff3', '기온_C_diff24', '기온_C_lag1', '기온_C_lag3', '기온_C_lag24', '풍속_ms_diff1', \n                '풍속_ms_diff3', '풍속_ms_diff24', '풍속_ms_lag1', '풍속_ms_lag3', '풍속_ms_lag24', '습도_pct_diff1',\n                '습도_pct_diff3', '습도_pct_diff24', '습도_pct_lag1', '습도_pct_lag3', '습도_pct_lag24', '기온_C_mean3', \n                '기온_C_median3', '기온_C_range3', '기온_C_median_mean_diff3', '기온_C_mean24', '기온_C_median24', '기온_C_range24', \n                '기온_C_median_mean_diff24', '풍속_ms_mean3', '풍속_ms_median3', '풍속_ms_range3', '풍속_ms_median_mean_diff3', \n                '풍속_ms_mean24', '풍속_ms_median24', '풍속_ms_range24', '풍속_ms_median_mean_diff24', '습도_pct_mean3', '습도_pct_median3', \n                '습도_pct_range3', '습도_pct_median_mean_diff3', '습도_pct_mean24', '습도_pct_median24', '습도_pct_range24', \n                '습도_pct_median_mean_diff24']\n\nfeature_cols = [col for col in train_df.columns if col not in exclude_cols + [target_col]]\nprint(f\"Selected features: {feature_cols}\")\nprint(f\"Number of features: {len(feature_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# K fold\n\ntrain_df['일시'] = pd.to_datetime(train_df['일시'])\nsplit_date = pd.Timestamp('2024-08-18') # 마지막 주차\nunique_types = train_df['건물유형'].unique()\nn_splits = len(unique_types)\n\nrng = np.random.default_rng(42)\nunique_types = np.array(unique_types)\nrng.shuffle(unique_types)\ntype_folds = np.array_split(unique_types, n_splits)\n\nX_test = test_df[feature_cols]\n\n# 출력 디렉토리 생성\nout_dir = Path('./cv_outputs')\nout_dir.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# optuna 정의\ndef cv_objective(trial):\n    param = {\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n        'gamma': trial.suggest_float('gamma', 1e-3, 10.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.9, 1.0),\n        'subsample': trial.suggest_categorical('subsample', [0.7, 0.8, 0.9, 1.0]),\n        'max_depth': trial.suggest_categorical('max_depth', [11, 12, 13, 14, 15, 16, 17, 18, 19]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 20, 100),\n        'eta': trial.suggest_float('eta', 0.01, 0.3),\n    }\n    fold_scores = []\n    \n    for fold_idx in range(n_splits):\n        valid_types = set(type_folds[fold_idx])\n        valid_mask = train_df['건물유형'].isin(valid_types) & (train_df['일시'] >= split_date)\n        train_mask = (~train_df['건물유형'].isin(valid_types)) | (train_df['일시'] < split_date)\n        \n        if valid_mask.sum() == 0:\n            continue\n            \n        X_tr = train_df.loc[train_mask, feature_cols]\n        y_tr = train_df.loc[train_mask, target_col]\n        X_va = train_df.loc[valid_mask, feature_cols]\n        y_va = train_df.loc[valid_mask, target_col]\n        \n        model = XGBRegressor(\n            n_estimators=10000,\n            learning_rate=param['eta'],\n            max_depth=param['max_depth'],\n            subsample=param['subsample'],\n            colsample_bytree=param['colsample_bytree'],\n            min_child_weight=param['min_child_weight'],\n            reg_alpha=param['reg_alpha'],\n            reg_lambda=param['reg_lambda'],\n            gamma=param['gamma'],\n            objective=weighted_mse(3),\n            random_state=42,\n            device=\"cuda\",\n            tree_method=\"hist\",\n            enable_categorical=True,\n            early_stopping_rounds=50,\n        )\n        \n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            verbose=0\n        )\n        \n        y_pred_val = model.predict(X_va)\n        smape_score = SMAPE(y_va, y_pred_val)\n        fold_scores.append(smape_score)\n        \n        del model\n        gc.collect()\n    \n    if len(fold_scores) == 0:\n        return float('inf')\n    \n    return np.mean(fold_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optuna 하이퍼파라미터 최적화\nprint(f\"\\n=== Starting Global Optuna Hyperparameter Optimization ===\")\nstudy = optuna.create_study(\n    direction='minimize',\n    pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=30)\n)\n\nstudy.optimize(\n    cv_objective,\n    n_trials=None,\n    timeout=3600*11,\n    n_jobs=8,\n    gc_after_trial=True,\n    show_progress_bar=True\n)\n\nprint(\"=== Global Optuna Results ===\")\nprint(f\"Best CV SMAPE: {study.best_value:.4f}\")\nprint(\"Best parameters:\")\nfor key, value in study.best_params.items():\n    print(f\"  {key}: {value}\")\n\n# Optuna 히스토리 저장\ndf_trials = study.trials_dataframe().sort_values(by=['value'], ascending=[True]).reset_index(drop=True)\noptuna_csv_path = './optuna_trials_global_cv.csv'\ndf_trials.to_csv(optuna_csv_path, index=False)\nprint(f\"Optuna trials saved to: {optuna_csv_path}\")\n\n\n# 상위 3개 하이퍼 파라미터 저장\nparam_columns = ['reg_lambda', 'gamma', 'reg_alpha', 'colsample_bytree', 'subsample', 'max_depth', 'min_child_weight', 'eta']\ntop3_trials = df_trials.head(3).copy()\n\n# 파라미터 컬럼들을 찾아서 추출 (params_ prefix가 있는 경우)\nparam_cols_in_df = [col for col in df_trials.columns if any(param in col for param in param_columns)]\nif not param_cols_in_df:\n    # params_ prefix가 있는 경우\n    param_cols_in_df = [col for col in df_trials.columns if col.startswith('params_')]\n\n# 상위 3개의 파라미터만 추출\nbest_3_params_data = []\nfor idx, row in top3_trials.iterrows():\n    param_dict = {'rank': len(best_3_params_data) + 1, 'cv_score': row['value']}\n    \n    # 파라미터 추출\n    for param_name in param_columns:\n        # params_{param_name} 컬럼에서 값 추출\n        col_name = f'params_{param_name}'\n        if col_name in df_trials.columns:\n            param_dict[param_name] = row[col_name]\n        else:\n            # 대안으로 user_attrs에서 찾기\n            for col in df_trials.columns:\n                if param_name in col:\n                    param_dict[param_name] = row[col]\n                    break\n    \n    best_3_params_data.append(param_dict)\n\n# Best 3 params CSV 저장\nbest_3_params_df = pd.DataFrame(best_3_params_data)\nbest_3_params_path = './best_3_optuna_params.csv'\nbest_3_params_df.to_csv(best_3_params_path, index=False)\nprint(f\"Best 3 parameters saved to: {best_3_params_path}\")\nprint(\"\\n=== Top 3 Parameter Sets ===\")\nprint(best_3_params_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\n=== Training with each of the top 3 parameter sets ===\")\n\nall_param_results = []  # 전체 결과 저장용\nall_test_predictions = []  # 테스트 예측 저장용\n\"\"\"\n# best 1 \n{\n    'colsample_bytree': 0.9506169035060151,\n    'eta': 0.030648887064687796,\n    'gamma': 0.09894819199058635,\n    'max_depth': 14,\n    'min_child_weight': 83,\n    'reg_alpha': 1.4459133024977084,\n    'reg_lambda': 5.55055188250579,\n    'subsample': 1\n}\n\n# best 2\n{\n    'colsample_bytree': 0.951206460894219,\n    'eta': 0.0320356578612075,\n    'gamma': 0.051206765719055,\n    'max_depth': 14,\n    'min_child_weight': 81,\n    'reg_alpha': 1.47674401806305,\n    'reg_lambda': 5.42854453302006,\n    'subsample': 1\n}\n\n# best 3\n{\n    'colsample_bytree': 0.948943295392428,\n    'eta': 0.0329996779509758,\n    'gamma': 0.314764846013268,\n    'max_depth': 14,\n    'min_child_weight': 80,\n    'reg_alpha': 0.946560191231365,\n    'reg_lambda': 5.2825285099683,\n    'subsample': 0.7\n}\"\"\"\n\nfor param_idx, param_row in best_3_params_df.iterrows():\n    print(f\"\\n--- Training with Parameter Set {param_idx + 1} (Rank {int(param_row['rank'])}) ---\")\n\n    current_params = {\n        'reg_lambda': param_row['reg_lambda'],\n        'gamma': param_row['gamma'],\n        'reg_alpha': param_row['reg_alpha'],\n        'colsample_bytree': param_row['colsample_bytree'],\n        'subsample': param_row['subsample'],\n        'max_depth': int(param_row['max_depth']),\n        'min_child_weight': int(param_row['min_child_weight']),\n        'eta': param_row['eta'],\n    }\n\n    param_fold_metrics = []\n    param_test_preds = []\n    param_fold_summaries = []\n    \n    # K-Fold 학습\n    for fold_idx in range(n_splits):\n        valid_types = set(type_folds[fold_idx])\n        valid_mask = train_df['건물유형'].isin(valid_types) & (train_df['일시'] >= split_date)\n        train_mask = (~train_df['건물유형'].isin(valid_types)) | (train_df['일시'] < split_date)\n        \n        if valid_mask.sum() == 0:\n            continue\n            \n        X_tr = train_df.loc[train_mask, feature_cols]\n        y_tr = train_df.loc[train_mask, target_col]\n        X_va = train_df.loc[valid_mask, feature_cols]\n        y_va = train_df.loc[valid_mask, target_col]\n        \n        # 현재 파라미터로 모델 학습\n        model = XGBRegressor(\n            n_estimators=10000,\n            learning_rate=current_params['eta'],\n            max_depth=current_params['max_depth'],\n            subsample=current_params['subsample'],\n            colsample_bytree=current_params['colsample_bytree'],\n            min_child_weight=current_params['min_child_weight'],\n            reg_alpha=current_params['reg_alpha'],\n            reg_lambda=current_params['reg_lambda'],\n            gamma=current_params['gamma'],\n            objective=weighted_mse(3),\n            random_state=42,\n            device=\"cuda\",\n            tree_method=\"hist\",\n            enable_categorical=True,\n            early_stopping_rounds=100,\n        )\n        \n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            verbose=100 if param_idx == 0 else 0  # 첫 번째 파라미터만 verbose\n        )\n        \n        # 검증 성능 계산\n        va_pred = model.predict(X_va)\n        smape = SMAPE(y_va, va_pred)\n        inv_smape = SMAPE(inv_boxcox(y_va, lam, boxcox_offset), inv_boxcox(va_pred, lam, boxcox_offset))\n        mae = mean_absolute_error(y_va, va_pred)\n        rmse = np.sqrt(mean_squared_error(y_va, va_pred))\n        \n        param_fold_metrics.append((smape, inv_smape, mae, rmse))\n        \n        # 폴드 요약 저장\n        param_fold_summaries.append({\n            'param_rank': int(param_row['rank']),\n            'fold': fold_idx + 1,\n            'valid_types': ','.join(map(str, sorted(list(valid_types)))),\n            'valid_samples': int(len(X_va)),\n            'train_samples': int(len(X_tr)),\n            'smape': float(smape),\n            'inv_smape': float(inv_smape),\n            'mae': float(mae),\n            'rmse': float(rmse),\n        })\n        \n        print(f\"  [Fold {fold_idx+1}] SMAPE: {smape:.4f} | inv_SMAPE: {inv_smape:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n        \n        # 테스트 예측\n        te_pred = model.predict(X_test)\n        te_pred_inv = inv_boxcox(te_pred, lam, boxcox_offset)\n        param_test_preds.append(te_pred_inv)\n        \n        del model\n        gc.collect()\n    \n    # 현재 파라미터 셋의 전체 성능 요약\n    if param_fold_metrics:\n        metrics_arr = np.array(param_fold_metrics)\n        avg_smape = metrics_arr[:, 0].mean()\n        std_smape = metrics_arr[:, 0].std()\n        avg_inv_smape = metrics_arr[:, 1].mean()\n        std_inv_smape = metrics_arr[:, 1].std()\n        avg_mae = metrics_arr[:, 2].mean()\n        avg_rmse = metrics_arr[:, 3].mean()\n        \n        print(f\"  Parameter Set {param_idx + 1} Summary:\")\n        print(f\"    SMAPE: {avg_smape:.4f} ± {std_smape:.4f}\")\n        print(f\"    inv_SMAPE: {avg_inv_smape:.4f} ± {std_inv_smape:.4f}\")\n        print(f\"    MAE: {avg_mae:.4f}\")\n        print(f\"    RMSE: {avg_rmse:.4f}\")\n        \n        # 전체 결과에 추가\n        param_result = {\n            'param_rank': int(param_row['rank']),\n            'cv_score_optuna': param_row['cv_score'],\n            'actual_cv_smape_mean': avg_smape,\n            'actual_cv_smape_std': std_smape,\n            'actual_cv_inv_smape_mean': avg_inv_smape,\n            'actual_cv_inv_smape_std': std_inv_smape,\n            'actual_cv_mae_mean': avg_mae,\n            'actual_cv_rmse_mean': avg_rmse,\n            **current_params\n        }\n        all_param_results.append(param_result)\n        \n        # 전체 폴드 요약을 all_param_results에 추가\n        all_param_results.extend(param_fold_summaries)\n    \n    # 테스트 예측 앙상블 (현재 파라미터 셋)\n    if param_test_preds:\n        test_pred_ensemble = np.mean(np.column_stack(param_test_preds), axis=1)\n        all_test_predictions.append({\n            'param_rank': int(param_row['rank']),\n            'predictions': test_pred_ensemble\n        })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 결과 저장\nprint(f\"\\n=== Saving Results ===\")\n\n# 파라미터별 성능 요약 저장\nparam_performance_df = pd.DataFrame([r for r in all_param_results if 'fold' not in r])\nparam_performance_path = './top3_params_performance_summary.csv'\nparam_performance_df.to_csv(param_performance_path, index=False)\nprint(f\"Parameter performance summary saved to: {param_performance_path}\")\n\n# 파라미터별 폴드별 상세 결과 저장  \nparam_fold_details_df = pd.DataFrame([r for r in all_param_results if 'fold' in r])\nparam_fold_details_path = './top3_params_fold_details.csv'\nparam_fold_details_df.to_csv(param_fold_details_path, index=False)\nprint(f\"Parameter fold details saved to: {param_fold_details_path}\")\n\n# 각 파라미터별 테스트 예측 저장\nfor pred_data in all_test_predictions:\n    rank = pred_data['param_rank']\n    predictions = pred_data['predictions']\n    \n    # 제출 형식으로 저장\n    submission_param = pd.DataFrame({\n        'num_date_time': test_df['num_date_time'],\n        'answer': predictions\n    })\n    submission_param = sub_df[['num_date_time']].merge(submission_param, on='num_date_time', how='left')\n    \n    submission_path = f'./submission_param_rank_{rank}.csv'\n    submission_param.to_csv(submission_path, index=False)\n    print(f\"Submission for parameter rank {rank} saved to: {submission_path}\")\n\n# 상위 3개 파라미터의 앙상블 예측도 생성\nif len(all_test_predictions) >= 2:\n    ensemble_predictions = np.mean([pred['predictions'] for pred in all_test_predictions], axis=0)\n    \n    submission_ensemble = pd.DataFrame({\n        'num_date_time': test_df['num_date_time'],\n        'answer': ensemble_predictions\n    })\n    submission_ensemble = sub_df[['num_date_time']].merge(submission_ensemble, on='num_date_time', how='left')\n    \n    ensemble_path = './submission_top3_ensemble.csv'\n    submission_ensemble.to_csv(ensemble_path, index=False)\n    print(f\"Top 3 ensemble submission saved to: {ensemble_path}\")\n\nprint(f\"\\n=== Final Summary ===\")\nprint(\"Top 3 Parameter Performance:\")\nprint(param_performance_df[['param_rank', 'cv_score_optuna', 'actual_cv_smape_mean', 'actual_cv_inv_smape_mean']].to_string(index=False))\n\nprint(\"\\n=== All files saved successfully! ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:03.557840Z","iopub.execute_input":"2025-08-27T08:14:03.558124Z","iopub.status.idle":"2025-08-27T08:26:23.829641Z","shell.execute_reply.started":"2025-08-27T08:14:03.558100Z","shell.execute_reply":"2025-08-27T08:26:23.829031Z"}},"outputs":[{"name":"stdout","text":"pandas version: 2.2.3\nnumpy version: 1.26.4\noptuna version: 4.4.0\nxgboost version: 2.0.3\nTrain shape: (203943, 97)\nTest shape: (16800, 96)\nSubmission shape: (16800, 2)\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-08-27 08:14:11,580] A new study created in memory with name: no-name-136f9e30-e319-45a3-989f-545903af403d\n","output_type":"stream"},{"name":"stdout","text":"Selected features: ['건물번호', '기온(°C)', '강수량(mm)', '풍속(m/s)', '습도(%)', '건물유형', '연면적(m2)', '냉방면적(m2)', '태양광용량(kW)', 'ESS저장용량(kWh)', 'PCS용량(kW)', '연면적구간', '냉방면적구간', 'month', 'week_of_year', 'dow', 'day', 'hour', 'is_holiday', 'is_building_holiday', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'dow_sin', 'dow_cos', 'peak_flag', 'cooling_ratio', 'temp_cool_ratio', 'heat_index', 'CDD', 'discomfort_index', 'discomfort_category', 'heat_wave_flag', 'tropical_night_flag', 'feels_like_temp', 'wet_bulb', 'dow_hour_mean', 'hour_mean', 'hour_std', 'cluster_id']\nNumber of features: 43\n\n=== Starting Global Optuna Hyperparameter Optimization ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   0%|          | 00:00/05:00","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4a0b98716074691ad9110490b3a8b47"}},"metadata":{}},{"name":"stdout","text":"[I 2025-08-27 08:16:08,739] Trial 0 finished with value: 1.3460565254022039 and parameters: {'reg_lambda': 6.057287516569718, 'gamma': 2.26245997086395, 'reg_alpha': 2.370207024975265, 'colsample_bytree': 0.9423327236118363, 'subsample': 0.8, 'max_depth': 19, 'min_child_weight': 42, 'eta': 0.08228552323536995}. Best is trial 0 with value: 1.3460565254022039.\n[I 2025-08-27 08:17:35,516] Trial 1 finished with value: 1.4182775077272982 and parameters: {'reg_lambda': 8.66653670098045, 'gamma': 8.695611345090008, 'reg_alpha': 9.232872661211694, 'colsample_bytree': 0.9234838724401777, 'subsample': 0.7, 'max_depth': 19, 'min_child_weight': 40, 'eta': 0.18539628234244288}. Best is trial 0 with value: 1.3460565254022039.\n[I 2025-08-27 08:19:02,232] Trial 2 finished with value: 1.3816527385104487 and parameters: {'reg_lambda': 7.449041961632654, 'gamma': 6.2190883691291, 'reg_alpha': 3.4360132655427007, 'colsample_bytree': 0.9966843914081787, 'subsample': 0.9, 'max_depth': 12, 'min_child_weight': 68, 'eta': 0.18916602084519143}. Best is trial 0 with value: 1.3460565254022039.\n[I 2025-08-27 08:20:52,351] Trial 3 finished with value: 1.380864859376466 and parameters: {'reg_lambda': 4.150424443771857, 'gamma': 0.7918431456742121, 'reg_alpha': 3.854439518550354, 'colsample_bytree': 0.9176423301419933, 'subsample': 0.8, 'max_depth': 18, 'min_child_weight': 88, 'eta': 0.16700607145898408}. Best is trial 0 with value: 1.3460565254022039.\n=== Global Optuna Results ===\nBest CV SMAPE: 1.3461\nBest parameters:\n  reg_lambda: 6.057287516569718\n  gamma: 2.26245997086395\n  reg_alpha: 2.370207024975265\n  colsample_bytree: 0.9423327236118363\n  subsample: 0.8\n  max_depth: 19\n  min_child_weight: 42\n  eta: 0.08228552323536995\nOptuna trials saved to: ./optuna_trials_global_cv.csv\nBest 3 parameters saved to: ./best_3_optuna_params.csv\n\n=== Top 3 Parameter Sets ===\n   rank  cv_score  reg_lambda     gamma  reg_alpha  colsample_bytree  \\\n0     1  1.346057    6.057288  2.262460   2.370207          0.942333   \n1     2  1.380865    4.150424  0.791843   3.854440          0.917642   \n2     3  1.381653    7.449042  6.219088   3.436013          0.996684   \n\n   subsample  max_depth  min_child_weight       eta  \n0        0.8         19                42  0.082286  \n1        0.8         18                88  0.167006  \n2        0.9         12                68  0.189166  \n\n=== Training with each of the top 3 parameter sets ===\n\n--- Training with Parameter Set 1 (Rank 1) ---\n[0]\tvalidation_0-rmse:11.35651\n[99]\tvalidation_0-rmse:0.20984\n  [Fold 1] SMAPE: 1.5615 | inv_SMAPE: 5.6614 | MAE: 0.1581 | RMSE: 0.2091\n[0]\tvalidation_0-rmse:13.54074\n[99]\tvalidation_0-rmse:0.40858\n  [Fold 2] SMAPE: 1.6606 | inv_SMAPE: 7.1985 | MAE: 0.2448 | RMSE: 0.4086\n[0]\tvalidation_0-rmse:14.39058\n[99]\tvalidation_0-rmse:0.70843\n  [Fold 3] SMAPE: 2.4317 | inv_SMAPE: 10.7802 | MAE: 0.3970 | RMSE: 0.6287\n[0]\tvalidation_0-rmse:13.66041\n[99]\tvalidation_0-rmse:0.44967\n  [Fold 4] SMAPE: 1.6272 | inv_SMAPE: 6.7224 | MAE: 0.2281 | RMSE: 0.4496\n[0]\tvalidation_0-rmse:14.68934\n[99]\tvalidation_0-rmse:0.27507\n  [Fold 5] SMAPE: 1.1612 | inv_SMAPE: 5.0643 | MAE: 0.1756 | RMSE: 0.2748\n[0]\tvalidation_0-rmse:15.60036\n[99]\tvalidation_0-rmse:0.18371\n  [Fold 6] SMAPE: 0.7161 | inv_SMAPE: 3.2647 | MAE: 0.1253 | RMSE: 0.1835\n[0]\tvalidation_0-rmse:13.54124\n[99]\tvalidation_0-rmse:0.36319\n  [Fold 7] SMAPE: 1.6066 | inv_SMAPE: 6.9936 | MAE: 0.2410 | RMSE: 0.3575\n[0]\tvalidation_0-rmse:12.77345\n[99]\tvalidation_0-rmse:0.37941\n  [Fold 8] SMAPE: 1.4594 | inv_SMAPE: 6.1911 | MAE: 0.2054 | RMSE: 0.3794\n[0]\tvalidation_0-rmse:13.82413\n[99]\tvalidation_0-rmse:0.20181\n  [Fold 9] SMAPE: 0.7928 | inv_SMAPE: 3.4551 | MAE: 0.1175 | RMSE: 0.2008\n[0]\tvalidation_0-rmse:18.75989\n[99]\tvalidation_0-rmse:0.14365\n  [Fold 10] SMAPE: 0.4434 | inv_SMAPE: 2.0670 | MAE: 0.0859 | RMSE: 0.1341\n  Parameter Set 1 Summary:\n    SMAPE: 1.3461 ± 0.5510\n    inv_SMAPE: 5.7398 ± 2.3574\n    MAE: 0.1979\n    RMSE: 0.3226\n\n--- Training with Parameter Set 2 (Rank 2) ---\n  [Fold 1] SMAPE: 1.5481 | inv_SMAPE: 5.6660 | MAE: 0.1596 | RMSE: 0.2101\n  [Fold 2] SMAPE: 1.6673 | inv_SMAPE: 7.2318 | MAE: 0.2468 | RMSE: 0.4127\n  [Fold 3] SMAPE: 2.5075 | inv_SMAPE: 11.1072 | MAE: 0.4081 | RMSE: 0.6369\n  [Fold 4] SMAPE: 1.6280 | inv_SMAPE: 6.7331 | MAE: 0.2291 | RMSE: 0.4519\n  [Fold 5] SMAPE: 1.1800 | inv_SMAPE: 5.1256 | MAE: 0.1763 | RMSE: 0.2871\n  [Fold 6] SMAPE: 0.7429 | inv_SMAPE: 3.3914 | MAE: 0.1309 | RMSE: 0.1909\n  [Fold 7] SMAPE: 1.6216 | inv_SMAPE: 7.0682 | MAE: 0.2444 | RMSE: 0.3613\n  [Fold 8] SMAPE: 1.4588 | inv_SMAPE: 6.1814 | MAE: 0.2057 | RMSE: 0.3919\n  [Fold 9] SMAPE: 0.8487 | inv_SMAPE: 3.7057 | MAE: 0.1266 | RMSE: 0.2024\n  [Fold 10] SMAPE: 0.6059 | inv_SMAPE: 2.8306 | MAE: 0.1184 | RMSE: 0.1919\n  Parameter Set 2 Summary:\n    SMAPE: 1.3809 ± 0.5325\n    inv_SMAPE: 5.9041 ± 2.2849\n    MAE: 0.2046\n    RMSE: 0.3337\n\n--- Training with Parameter Set 3 (Rank 3) ---\n  [Fold 1] SMAPE: 1.6286 | inv_SMAPE: 5.9659 | MAE: 0.1683 | RMSE: 0.2228\n  [Fold 2] SMAPE: 1.7333 | inv_SMAPE: 7.5101 | MAE: 0.2568 | RMSE: 0.4470\n  [Fold 3] SMAPE: 2.3658 | inv_SMAPE: 10.4980 | MAE: 0.3883 | RMSE: 0.6344\n  [Fold 4] SMAPE: 1.6605 | inv_SMAPE: 6.8730 | MAE: 0.2336 | RMSE: 0.4563\n  [Fold 5] SMAPE: 1.1980 | inv_SMAPE: 5.2150 | MAE: 0.1797 | RMSE: 0.2758\n  [Fold 6] SMAPE: 0.7498 | inv_SMAPE: 3.4128 | MAE: 0.1303 | RMSE: 0.1867\n  [Fold 7] SMAPE: 1.5811 | inv_SMAPE: 6.8599 | MAE: 0.2341 | RMSE: 0.3410\n  [Fold 8] SMAPE: 1.4977 | inv_SMAPE: 6.3539 | MAE: 0.2109 | RMSE: 0.3897\n  [Fold 9] SMAPE: 0.8454 | inv_SMAPE: 3.6825 | MAE: 0.1252 | RMSE: 0.2142\n  [Fold 10] SMAPE: 0.5562 | inv_SMAPE: 2.5694 | MAE: 0.1046 | RMSE: 0.1502\n  Parameter Set 3 Summary:\n    SMAPE: 1.3817 ± 0.5183\n    inv_SMAPE: 5.8941 ± 2.2018\n    MAE: 0.2032\n    RMSE: 0.3318\n\n=== Saving Results ===\nParameter performance summary saved to: ./top3_params_performance_summary.csv\nParameter fold details saved to: ./top3_params_fold_details.csv\nSubmission for parameter rank 1 saved to: ./submission_param_rank_1.csv\nSubmission for parameter rank 2 saved to: ./submission_param_rank_2.csv\nSubmission for parameter rank 3 saved to: ./submission_param_rank_3.csv\nTop 3 ensemble submission saved to: ./submission_top3_ensemble.csv\n\n=== Final Summary ===\nTop 3 Parameter Performance:\n param_rank  cv_score_optuna  actual_cv_smape_mean  actual_cv_inv_smape_mean\n          1         1.346057              1.346057                  5.739839\n          2         1.380865              1.380865                  5.904116\n          3         1.381653              1.381653                  5.894052\n\n=== All files saved successfully! ===\n","output_type":"stream"}],"execution_count":null}]}